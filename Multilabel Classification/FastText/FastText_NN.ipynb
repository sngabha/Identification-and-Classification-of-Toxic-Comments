{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hK1PhsqSwl68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, Bidirectional\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yzNeVH_zxG1"
   },
   "outputs": [],
   "source": [
    "# Train test split ratio of 0.3\n",
    "X_train = pd.read_pickle(\"../../Preprocessing/Data/X_train.pkl\")\n",
    "X_test = pd.read_pickle(\"../../Preprocessing/Data/X_test.pkl\")\n",
    "y_train = pd.read_pickle(\"../../Preprocessing/Data/y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"../../Preprocessing/Data/y_test.pkl\")\n",
    "\n",
    "# Url - https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip \n",
    "# pre-trained FastText vector file\n",
    "# word million word vectors trained on Common Crawl (600B tokens)\n",
    "embedding_path = \"Data/crawl-300d-2M.vec\"\n",
    "\n",
    "# no. of dimensions for each vector\n",
    "embed_size = 300\n",
    "# maximum number of unique words to be considered\n",
    "max_features = 100000\n",
    "# maximum length of a comment\n",
    "max_len = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXnFUKaughsh"
   },
   "outputs": [],
   "source": [
    "# vectorize a text corpus \n",
    "# turns text into sequence of space-separated sequence of words\n",
    "# sequences are split into list of tokens\n",
    "# they will be indexed or vectorized\n",
    "\n",
    "# create tokenizer\n",
    "token = Tokenizer(num_words=max_features, lower=True)\n",
    "# fit the tokenizer on training data\n",
    "# create internal vocabulary index based on word frequency\n",
    "token.fit_on_texts(X_train)\n",
    "\n",
    "# convert comment texts to their numeric counterparts\n",
    "# transform each text in texts to sequence of integers\n",
    "# i.e replaces each word in text with corresponding integer value from \n",
    "# word_index dictionary\n",
    "X_train = token.texts_to_sequences(X_train)\n",
    "X_test = token.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-duymWIlIJN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txIki5xanDpL"
   },
   "outputs": [],
   "source": [
    "# padding to make comments uniform in length\n",
    "# output will be padded sequence of numbers\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bltdfyFqoW1s"
   },
   "outputs": [],
   "source": [
    "# load FastText word embeddings \n",
    "def get_coefficients(word,*arr): \n",
    "  return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefficients(*v.strip().split(\" \")) for v in open(embedding_path))\n",
    "\n",
    "# create embedding matrix that contains words in our corpus and their\n",
    "# corresponding values from FastText embeddings\n",
    "word_index = token.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features: \n",
    "      continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "      embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ka-DkSYwqxBi"
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            # results = self.model.evaluate(self.X_val, self.y_val, verbose=1)\n",
    "            # precision, recall, fscore, _ = precision_recall_fscore_support(y_pred, self.y_val)\n",
    "            # print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f} - score: {:.6f} - score: {:.6f} - score: {:.6f}\".format(epoch+1, score, precision, recall, fscore))\n",
    "            # print(\"\\n Results: test_loss: {:d} - test_accuracy: {:.6f}\".format(results[0], results[1]))\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlkCkehSq0cm"
   },
   "outputs": [],
   "source": [
    "# calculating ROC_AUC score after every epoch\n",
    "ra_val = RocAucEvaluation(validation_data=(X_test, y_test), interval=1)\n",
    "# stop training when a monitored quantity (specified by monitor attribute) has stopped improving\n",
    "# patience = number of epochs that produced the monitored quantity with no improvement after which training will be stopped\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hx5TD4Lyt16r"
   },
   "outputs": [],
   "source": [
    "def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
    "  # instantiates a Keras tensor\n",
    "  inp = Input(shape=(max_len,))\n",
    "  # first layer\n",
    "  # matrix is used to initialize weights in the Embedding layer of the model\n",
    "  # trainable=False to prevent the weights from being updated during training\n",
    "  x = Embedding(nb_words, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "  # drops entire 1D feature maps (channels) instead of individual elements\n",
    "  # randomly setting a fraction rate (dr) of input units to 0 at each update, to prevent overfitting\n",
    "  x1 = SpatialDropout1D(dr)(x)\n",
    "  \n",
    "  # bi-directional GRU and LSTM to keep contextual information in both directions\n",
    "  x = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "  x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "  y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "  y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "\n",
    "  # to minimize overfitting\n",
    "  avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "  max_pool1 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "  avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "  max_pool2 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "  # returns a single tensor by merging all the pooling layers\n",
    "  x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "  # Output layer which classifies a given comment into one of 6 toxic levels\n",
    "  x = Dense(6, activation = \"sigmoid\")(x)\n",
    "\n",
    "  # this model includes all layers required in computation of x given by inp\n",
    "  model = Model(inputs = inp, outputs = x)\n",
    "  # configure model for training\n",
    "  model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "  # trains the model for fixed number of epochs\n",
    "  history = model.fit(X_train, y_train, batch_size = 128, epochs = 3, validation_data = (X_test, y_test), \n",
    "                      verbose = 1, callbacks = [ra_val, early_stop])\n",
    "  # model = load_model(file_path)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "3fXHFsR4yPY2",
    "outputId": "a6beba9d-63c4-4cc4-ceb4-da48f0bbee08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39912 samples, validate on 19659 samples\n",
      "Epoch 1/3\n",
      "39912/39912 [==============================] - 397s 10ms/step - loss: 0.2412 - acc: 0.9144 - val_loss: 0.1320 - val_acc: 0.9510\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.931762\n",
      "Epoch 2/3\n",
      "39912/39912 [==============================] - 387s 10ms/step - loss: 0.1290 - acc: 0.9505 - val_loss: 0.1192 - val_acc: 0.9536\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.942917\n",
      "Epoch 3/3\n",
      "39912/39912 [==============================] - 387s 10ms/step - loss: 0.1194 - acc: 0.9531 - val_loss: 0.1125 - val_acc: 0.9563\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.949765\n"
     ]
    }
   ],
   "source": [
    "# learning_rate for optimizer    = 1e-4\n",
    "# learning_rate decay            = 0\n",
    "# Output dimensionality for LSTM = 128\n",
    "# dropout_rate                   = 0.2\n",
    "model = build_model(lr = 1e-4, lr_d = 0, units = 128, dr = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OLLBCvV34u7"
   },
   "outputs": [],
   "source": [
    "pickle.dump(model.history, open('Models/FastText_NN.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "oeIDE3Mwb42a",
    "outputId": "6a285eca-e11b-4cbe-d98e-985fe2eab977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 220)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 220, 300)     18345300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 220, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 220, 256)     329472      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 220, 256)     439296      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 219, 64)      32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 219, 64)      32832       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 19,181,274\n",
      "Trainable params: 835,974\n",
      "Non-trainable params: 18,345,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "5DL_QDluEZ46",
    "outputId": "d6d76e8f-1864-4a4f-a660-11899be7200e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19659/19659 [==============================] - 250s 13ms/step\n",
      "Test Loss: 0.11250274048393247\n",
      "Test Accuracy: 0.9563049891847725\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of FastText_NN_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
